{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aSjvYieIKf62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dde309f2-ab8c-42c1-9f01-8b4a5904a1c0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nSoham Mukherjee\\nProfessor Mazidi\\nCS 4395\\n2/18/2023\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\"\"\"\n",
        "Soham Mukherjee\n",
        "Professor Mazidi\n",
        "CS 4395\n",
        "2/18/2023\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 3\n",
        "### Soham Mukherjee\n",
        "### Professor Mazidi\n",
        "### CS 4395\n",
        "### 2/18/2023"
      ],
      "metadata": {
        "id": "x21WXxkKKpPB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is WordNet?\n",
        "\n",
        "WordNet is a lexical database that catalogues the relations between words as well as their alternatives. It can be thought of as a thesaurus, as it is like a dictionary that also offers synonyms for similar-context words. Semantically equivalent words are often bundled into the same groups known as synsets, which are considered referentially similar for all intents and purposes. An example would be \"home\" and \"house\" as they can be used interchangeably, and thus fall under the same synset in theory. "
      ],
      "metadata": {
        "id": "BSfS3TW_K08O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a noun. Output all synsets. \n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "noun = 'car'\n",
        "wn.synsets(noun)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hE615yxULs56",
        "outputId": "9c98b6ec-b090-40e0-8017-64c219a55eb8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('car.n.01'),\n",
              " Synset('car.n.02'),\n",
              " Synset('car.n.03'),\n",
              " Synset('car.n.04'),\n",
              " Synset('cable_car.n.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Select one synset from the list of synsets. Extract its definition, usage examples, and lemmas.\n",
        "From your selected synset, traverse up the WordNet hierarchy as far as you can, outputting the\n",
        "synsets as you go. Write a couple of sentences observing the way that WordNet is organized for\n",
        "nouns.\n",
        "\"\"\"\n",
        "selected_synset = wn.synset('car.n.01')\n",
        "\n",
        "definition = selected_synset.definition()\n",
        "examples = selected_synset.examples()\n",
        "lemmas = selected_synset.lemma_names()\n",
        "\n",
        "print(\"Definition: \", definition)\n",
        "print(\"Examples:\", examples)\n",
        "print(\"Lemmas: \", lemmas)\n",
        "\n",
        "print(\"Traversing up the WordNet Hierarchy...\")\n",
        "while True:\n",
        "    print(selected_synset.name())\n",
        "    hypernyms = selected_synset.hypernyms()\n",
        "    if hypernyms:\n",
        "        selected_synset = hypernyms[0]\n",
        "    else:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isqey2DiL5jk",
        "outputId": "2f3597ae-bedf-4b9b-f098-5e5eb2720e72"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Definition:  a motor vehicle with four wheels; usually propelled by an internal combustion engine\n",
            "Examples: ['he needs a car to get to work']\n",
            "Lemmas:  ['car', 'auto', 'automobile', 'machine', 'motorcar']\n",
            "Traversing up the WordNet Hierarchy...\n",
            "car.n.01\n",
            "motor_vehicle.n.01\n",
            "self-propelled_vehicle.n.01\n",
            "wheeled_vehicle.n.01\n",
            "container.n.01\n",
            "instrumentality.n.03\n",
            "artifact.n.01\n",
            "whole.n.02\n",
            "object.n.01\n",
            "physical_entity.n.01\n",
            "entity.n.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Write a couple of sentences observing the way that WordNet is organized for nouns.\n",
        "\n",
        "At the top of the hierarchy, WordNet stores the most general categories. The lower the hierarchy, the more specific the given noun. The noun hierarchy is organized according to the \"ISA\" relationship, where from bottom up, the lower noun ISA instance of the upper noun. For instance, above we can see that *motor_vehicle* is a hypernym of *car*, indicating that *car* is a type of *motor_vehicle*."
      ],
      "metadata": {
        "id": "4Alpj0WVkImN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output the following (or an empty list if none exist): hypernyms, hyponyms, meronyms,\n",
        "# holonyms, antonym.\n",
        "\n",
        "selected_synset = wn.synset('car.n.01')\n",
        "\n",
        "# hypernyms\n",
        "print(\"Hypernyms:\")\n",
        "print(selected_synset.hypernyms())\n",
        "\n",
        "# hyponyms\n",
        "print(\"Hyponyms:\")\n",
        "print(selected_synset.hyponyms())\n",
        "\n",
        "# meronyms\n",
        "print(\"Meronyms:\")\n",
        "print(selected_synset.part_meronyms() + selected_synset.member_meronyms())\n",
        "\n",
        "# holonyms\n",
        "print(\"Holonyms:\")\n",
        "print(selected_synset.part_holonyms() + selected_synset.member_holonyms())\n",
        "\n",
        "# antonyms\n",
        "print(\"Antonyms:\")\n",
        "antonyms = []\n",
        "for lemma in selected_synset.lemmas():\n",
        "    for antonym in lemma.antonyms():\n",
        "        antonyms.append(antonym)\n",
        "print(antonyms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18QcJ34TvEZH",
        "outputId": "767e6015-e038-446c-c3a7-e6290dd45a1c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hypernyms:\n",
            "[Synset('motor_vehicle.n.01')]\n",
            "Hyponyms:\n",
            "[Synset('ambulance.n.01'), Synset('beach_wagon.n.01'), Synset('bus.n.04'), Synset('cab.n.03'), Synset('compact.n.03'), Synset('convertible.n.01'), Synset('coupe.n.01'), Synset('cruiser.n.01'), Synset('electric.n.01'), Synset('gas_guzzler.n.01'), Synset('hardtop.n.01'), Synset('hatchback.n.01'), Synset('horseless_carriage.n.01'), Synset('hot_rod.n.01'), Synset('jeep.n.01'), Synset('limousine.n.01'), Synset('loaner.n.02'), Synset('minicar.n.01'), Synset('minivan.n.01'), Synset('model_t.n.01'), Synset('pace_car.n.01'), Synset('racer.n.02'), Synset('roadster.n.01'), Synset('sedan.n.01'), Synset('sport_utility.n.01'), Synset('sports_car.n.01'), Synset('stanley_steamer.n.01'), Synset('stock_car.n.01'), Synset('subcompact.n.01'), Synset('touring_car.n.01'), Synset('used-car.n.01')]\n",
            "Meronyms:\n",
            "[Synset('accelerator.n.01'), Synset('air_bag.n.01'), Synset('auto_accessory.n.01'), Synset('automobile_engine.n.01'), Synset('automobile_horn.n.01'), Synset('buffer.n.06'), Synset('bumper.n.02'), Synset('car_door.n.01'), Synset('car_mirror.n.01'), Synset('car_seat.n.01'), Synset('car_window.n.01'), Synset('fender.n.01'), Synset('first_gear.n.01'), Synset('floorboard.n.02'), Synset('gasoline_engine.n.01'), Synset('glove_compartment.n.01'), Synset('grille.n.02'), Synset('high_gear.n.01'), Synset('hood.n.09'), Synset('luggage_compartment.n.01'), Synset('rear_window.n.01'), Synset('reverse.n.02'), Synset('roof.n.02'), Synset('running_board.n.01'), Synset('stabilizer_bar.n.01'), Synset('sunroof.n.01'), Synset('tail_fin.n.02'), Synset('third_gear.n.01'), Synset('window.n.02')]\n",
            "Holonyms:\n",
            "[]\n",
            "Antonyms:\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a verb. Output all synsets.\n",
        "verb = \"drive\"\n",
        "wn.synsets(verb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHx8chHuv1jV",
        "outputId": "b72a4950-ed0a-4073-fb93-7cae9376e63c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('drive.n.01'),\n",
              " Synset('drive.n.02'),\n",
              " Synset('campaign.n.02'),\n",
              " Synset('driveway.n.01'),\n",
              " Synset('drive.n.05'),\n",
              " Synset('drive.n.06'),\n",
              " Synset('drive.n.07'),\n",
              " Synset('drive.n.08'),\n",
              " Synset('drive.n.09'),\n",
              " Synset('drive.n.10'),\n",
              " Synset('drive.n.11'),\n",
              " Synset('drive.n.12'),\n",
              " Synset('drive.v.01'),\n",
              " Synset('drive.v.02'),\n",
              " Synset('drive.v.03'),\n",
              " Synset('force.v.06'),\n",
              " Synset('drive.v.05'),\n",
              " Synset('repel.v.01'),\n",
              " Synset('drive.v.07'),\n",
              " Synset('drive.v.08'),\n",
              " Synset('drive.v.09'),\n",
              " Synset('tug.v.02'),\n",
              " Synset('drive.v.11'),\n",
              " Synset('drive.v.12'),\n",
              " Synset('drive.v.13'),\n",
              " Synset('drive.v.14'),\n",
              " Synset('drive.v.15'),\n",
              " Synset('drive.v.16'),\n",
              " Synset('drive.v.17'),\n",
              " Synset('drive.v.18'),\n",
              " Synset('drive.v.19'),\n",
              " Synset('drive.v.20'),\n",
              " Synset('drive.v.21'),\n",
              " Synset('drive.v.22')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Select one synset from the list of synsets. Extract its definition, usage examples, and lemmas.\n",
        "From your selected synset, traverse up the WordNet hierarchy as far as you can, outputting the\n",
        "synsets as you go. Write a couple of sentences observing the way that WordNet is organized for\n",
        "verbs.\n",
        "\"\"\"\n",
        "selected_synset = wn.synset('drive.v.02')\n",
        "\n",
        "definition = selected_synset.definition()\n",
        "examples = selected_synset.examples()\n",
        "lemmas = selected_synset.lemma_names()\n",
        "\n",
        "print(\"Definition: \", definition)\n",
        "print(\"Examples:\", examples)\n",
        "print(\"Lemmas: \", lemmas)\n",
        "\n",
        "print(\"Traversing up the WordNet Hierarchy...\")\n",
        "while True:\n",
        "    print(selected_synset.name())\n",
        "    hypernyms = selected_synset.hypernyms()\n",
        "    if hypernyms:\n",
        "        selected_synset = hypernyms[0]\n",
        "    else:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzxp_CV2BjIA",
        "outputId": "ac599b99-b70d-4015-ee06-797ff03a1216"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Definition:  travel or be transported in a vehicle\n",
            "Examples: ['We drove to the university every morning', 'They motored to London for the theater']\n",
            "Lemmas:  ['drive', 'motor']\n",
            "Traversing up the WordNet Hierarchy...\n",
            "drive.v.02\n",
            "travel.v.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Write a couple of sentences observing the way that WordNet is organized for verbs.\n",
        "\n",
        "At the top of the hierarchy, WordNet stores the most general categories. The lower the hierarchy, the more specific the given verb. The verb hierarchy is also organized according to the \"ISA\" relationship, where from bottom up, the lower verb ISA instance of the upper verb. For instance, above we can see that *travel* is a hypernym of *drive*, indicating that *drive* is a type of *travel*."
      ],
      "metadata": {
        "id": "QXoz2ChaB3jk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use morphy to find as many different forms of the word as you can. \n",
        "word = 'drive'\n",
        "synsets = wn.synsets(word)\n",
        "\n",
        "# get all the inflected forms for each synset.\n",
        "inflected_forms = set()\n",
        "for synset in synsets:\n",
        "    for lemma in synset.lemmas():\n",
        "        inflected_forms.add(lemma.name())\n",
        "\n",
        "# print the set\n",
        "print(inflected_forms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2w-y0AacCQyD",
        "outputId": "a223cf24-733f-473f-8cbf-0cc8195578dd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'driving_force', 'force', 'private_road', 'crusade', 'effort', 'tug', 'movement', 'parkway', 'campaign', 'thrust', 'ram', 'force_back', 'drive', 'motor', 'beat_back', 'push', 'take', 'labor', 'push_back', 'driveway', 'labour', 'cause', 'aim', 'driving', 'ride', 'repulse', 'get', 'repel'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select two words that you think might be similar. Find the specific synsets you are interested in.\n",
        "# Run the Wu-Palmer similarity metric and the Lesk algorithm. Write a couple of sentences with\n",
        "# your observations\n",
        "\n",
        "# Wu-Palmer Algorithm\n",
        "word_1 = \"freezer\"\n",
        "word_2 = \"fridge\"\n",
        "\n",
        "w1_synsets = wn.synsets(word_1)\n",
        "w2_synsets = wn.synsets(word_2)\n",
        "\n",
        "similarity = w1_synsets[0].wup_similarity(w2_synsets[0])\n",
        "print(f\"Similarity of {word_1} and {word_2}: {similarity}\")\n",
        "\n",
        "# Lesk algorithm\n",
        "from nltk.wsd import lesk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "sentence_1 = [\"I\", \"like\", \"to\", \"put\", \"icecream\", \"in\", \"the\", \"freezer\"]\n",
        "sentence_2 = [\"Why\", \"does\", \"the\", \"fridge\", \"have\", \"my\", \"keys?\"]\n",
        "sense_1 = lesk(sentence_1, word_1, 'n')\n",
        "sense_2 = lesk(sentence_2, word_2, 'n')\n",
        "\n",
        "print(sense_1)\n",
        "print(sense_2)\n",
        "\n",
        "def_1 = sense_1.definition()\n",
        "def_2 = sense_2.definition()\n",
        "\n",
        "print(def_1)\n",
        "print(def_2)\n",
        "\n",
        "overlap = set(def_1.split()).intersection(set(def_2.split()))\n",
        "print(f\"Overlap between {word_1} and {word_2}: {overlap}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bv-UrUK8Cyth",
        "outputId": "93b9fb67-a49c-43d2-82a6-a5e94cdbdda1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity of freezer and fridge: 0.9629629629629629\n",
            "Synset('deep-freeze.n.01')\n",
            "Synset('electric_refrigerator.n.01')\n",
            "electric refrigerator (trade name Deepfreeze) in which food is frozen and stored for long periods of time\n",
            "a refrigerator in which the coolant is pumped around by an electric motor\n",
            "Overlap between freezer and fridge: {'refrigerator', 'in', 'is', 'electric', 'which'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write a couple of sentences with your observations.\n",
        "\n",
        "There is a significant amount of overlap between freezer and fridge, as Wu-Palmer yields a high similarity of ~0.9629.\n",
        "\n",
        "Using the Lesk Algorithm returns most notably 'refrigerator' and 'electric', both terms that refer to the fridge and freezer which are both electric cooling machines. \n",
        "\n",
        "With the above results, we can see that both a fridge and a freezer are very similar words - not only in spelling but also in word origin. "
      ],
      "metadata": {
        "id": "1EiGH1w2TQq8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write a couple of sentences about SentiWordNet, describing its functionality and possible use cases.\n",
        "\n",
        "SentiWordNet judges words based on their sentiment. It checks for strong opinions, primarily if a given data is positive, negative, or neutral when it comes to discerning the author's point of view. While WordNet gives purely binary scores (+ or -) for every word, SentiWordNet is continuous, giving positive and negative values in the range [0, 1]. SentiWordNet would probably be best for dissecting opinion articles. "
      ],
      "metadata": {
        "id": "FyKgTFtaHD2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select an emotionally charged word. Find its senti-synsets and output the polarity scores\n",
        "# for each word. Make up a sentence. Output the polarity for each word in the sentence. Write a\n",
        "# couple of sentences about your observations of the scores and the utility of knowing these\n",
        "# scores in an NLP application. \n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "\n",
        "strong_word = \"awful\"\n",
        "\n",
        "print(\"Senti-synsets: \")\n",
        "strong_synsets=list(swn.senti_synsets(strong_word))\n",
        "print(strong_synsets)\n",
        "print()\n",
        "\n",
        "print(\"Polarity Scores: \")\n",
        "for syn in strong_synsets:\n",
        "  print(syn)\n",
        "  print(\"Positive score = \", syn.pos_score())\n",
        "  print(\"Negative score = \", syn.neg_score())\n",
        "  print(\"Objective score = \", syn.obj_score())\n",
        "print()\n",
        "\n",
        "print(\"Polarity of words in a sentence: \")\n",
        "sent = \"I never want to see you here, there, or anywhere near my house again.\"\n",
        "print(sent)\n",
        "neg = 0\n",
        "pos = 0\n",
        "tokens = sent.split()\n",
        "for token in tokens:\n",
        "    syn_list = list(swn.senti_synsets(token))\n",
        "    if syn_list:\n",
        "        syn = syn_list[0]\n",
        "        neg += syn.neg_score()\n",
        "        pos += syn.pos_score()\n",
        "    \n",
        "print(\"neg\\tpos counts\")\n",
        "print(neg, '\\t', pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCldsLoUIKBr",
        "outputId": "8b21c710-7940-4da2-87cd-f613a3edcdd4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Senti-synsets: \n",
            "[SentiSynset('atrocious.s.02'), SentiSynset('awful.s.02'), SentiSynset('nasty.a.01'), SentiSynset('awed.s.01'), SentiSynset('frightful.s.02'), SentiSynset('amazing.s.02'), SentiSynset('terribly.r.01')]\n",
            "\n",
            "Polarity Scores: \n",
            "<atrocious.s.02: PosScore=0.0 NegScore=0.875>\n",
            "Positive score =  0.0\n",
            "Negative score =  0.875\n",
            "Objective score =  0.125\n",
            "<awful.s.02: PosScore=0.0 NegScore=0.625>\n",
            "Positive score =  0.0\n",
            "Negative score =  0.625\n",
            "Objective score =  0.375\n",
            "<nasty.a.01: PosScore=0.0 NegScore=0.875>\n",
            "Positive score =  0.0\n",
            "Negative score =  0.875\n",
            "Objective score =  0.125\n",
            "<awed.s.01: PosScore=0.5 NegScore=0.5>\n",
            "Positive score =  0.5\n",
            "Negative score =  0.5\n",
            "Objective score =  0.0\n",
            "<frightful.s.02: PosScore=0.125 NegScore=0.25>\n",
            "Positive score =  0.125\n",
            "Negative score =  0.25\n",
            "Objective score =  0.625\n",
            "<amazing.s.02: PosScore=0.875 NegScore=0.125>\n",
            "Positive score =  0.875\n",
            "Negative score =  0.125\n",
            "Objective score =  0.0\n",
            "<terribly.r.01: PosScore=0.25 NegScore=0.0>\n",
            "Positive score =  0.25\n",
            "Negative score =  0.0\n",
            "Objective score =  0.75\n",
            "\n",
            "Polarity of words in a sentence: \n",
            "I never want to see you here, there, or anywhere near my house again.\n",
            "neg\tpos counts\n",
            "0.875 \t 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write a couple of sentences about your observations of the scores and the utility of knowing these scores in an NLP application\n",
        "\n",
        "I notice that these scores have three types of scores - positive, negative, and objective scores. The lower the objective score, the more negative the word. Objective scores are determined by subtracting the negative score from 1. \n",
        "\n",
        "These scores would be useful for an NLP application because it could help researchers and people alike classify sentiment from a given text to help readers understand what type of article they're reading. Additionally, it could check for strong positive or negative bias in news media, and see if there's a way to maintain neutrality of the given information. "
      ],
      "metadata": {
        "id": "gNdfjwLyLQ3Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write a couple of sentences about what a collocation is.\n",
        "\n",
        "Collocations are two or more words that tend to appear together frequently; an example would be \"social media\". When combined, these words form a meaning that is normally different from the separate words. For instance, \"social media\" refers to the public posting of a creator's content on the internet. "
      ],
      "metadata": {
        "id": "uEkPlcziOduE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a couple of sentences about what a collocation is. Output collocations for text4, the\n",
        "# Inaugural corpus. Select one of the collocations identified by NLTK. Calculate mutual\n",
        "# information. Write commentary on the results of the mutual information formula and your\n",
        "# interpretation.\n",
        "from nltk.book import text4\n",
        "\n",
        "# output collocations for text4\n",
        "print(text4.collocations())\n",
        "\n",
        "# Select one of the collocations identified by NLTK. Calculate mutual information.\n",
        "text = ' '.join(text4.tokens)\n",
        "print(text[:50])\n",
        "\n",
        "import math\n",
        "vocab = len(set(text4))\n",
        "ow = text.count('Old World')/vocab\n",
        "print(\"p(Old World) = \",ow )\n",
        "o = text.count('Old')/vocab\n",
        "print(\"p(Old) = \", o)\n",
        "w = text.count('World')/vocab\n",
        "print('p(World) = ', w)\n",
        "pmi = math.log2(ow / (o * w))\n",
        "print('pmi = ', pmi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DHw-LIDR3um",
        "outputId": "98a4ebdf-bf5e-47b9-d925-1ffc9899f49c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "United States; fellow citizens; years ago; four years; Federal\n",
            "Government; General Government; American people; Vice President; God\n",
            "bless; Chief Justice; one another; fellow Americans; Old World;\n",
            "Almighty God; Fellow citizens; Chief Magistrate; every citizen; Indian\n",
            "tribes; public debt; foreign nations\n",
            "None\n",
            "p(Old World) =  0.000997506234413965\n",
            "p(Old) =  0.0010972568578553616\n",
            "p(World) =  0.0017955112219451373\n",
            "pmi =  8.983886091037398\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]   Package nps_chat is already up-to-date!\n",
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Package webtext is already up-to-date!\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write commentary on the results of the mutual information formula and your interpretation.\n",
        "\n",
        "The higher the Pointwise Mutual Information (PMI), the more likely the word is to be a collocation. It makes sense that the probability of the words separately is more than the probability of the words together as they can be treated as independent objects. My interpretation is that because \"old\" and \"world\" frequently appears as \"old world\" and that the probability is very close to the separate probabilities of the two words, this means that it's highly likely to be a collocation, hence the high PMI."
      ],
      "metadata": {
        "id": "5Ub-Oy3DVW6e"
      }
    }
  ]
}
